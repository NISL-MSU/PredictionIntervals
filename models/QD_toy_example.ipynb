{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# QD Loss Example\n",
    "\n",
    "In Section III.A of our paper, we discussed the method proposed by [Pearce et al. (2018)](https://arxiv.org/pdf/1802.07167.pdf) and argued that *\"minimizing $MPIW_{capt}$ does not imply that the width of the PIs generated for the non-captured samples will not decrease along with the width of the PIs generated for the captured samples.\"* Therefore, the aim of this notebook is to support this claim."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin by constructing a toy neural network with four learnable parameters such that $y^u = w_1*x + b_1$ and $y^\\ell = w_2*x + b_2$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ToyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyNet, self).__init__()\n",
    "        # Define the learnable parameters with initial values\n",
    "        self.w1 = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "        self.b1 = nn.Parameter(torch.tensor(5.0), requires_grad=True)\n",
    "\n",
    "        self.w2 = nn.Parameter(torch.tensor(1.5), requires_grad=True)\n",
    "        self.b2 = nn.Parameter(torch.tensor(0.2), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate yu and yl\n",
    "        yu = self.w1 * x + self.b1\n",
    "        yl = self.w2 * x + self.b2\n",
    "\n",
    "        return yu, yl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Recall that the loss function proposed by Pearce et al. is as follows:\n",
    "$$Loss_{QD} = MPIW_{capt}  + \\delta \\, \\frac{N}{\\alpha (1 - \\alpha)} \\max(0, (1 - \\alpha) - PICP) ^ 2$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$MPIW_{capt} = \\frac{1}{\\epsilon + \\sum_{i}k_i} \\sum_{i=1}^N (\\hat{y}^u_i - \\hat{y}^\\ell_i) \\, k_i$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$k_i =\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if $\\hat{y}^\\ell_i < \\hat{y}_i < \\, \\hat{y}^u_i$ and $\\hat{y}^\\ell_i < y_i < \\, \\hat{y}^u_i$}\\\\\n",
    "      0, & \\text{otherwise}.\n",
    "    \\end{cases}$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def QD_objective(y_pred, y_true, soften_=1, alpha_=0.05, beta_=0.03, device='cuda:0'):\n",
    "    \"\"\"Original Loss_QD-soft, adapted from https://github.com/TeaPearce/Deep_Learning_Prediction_Intervals\"\"\"\n",
    "    # Separate upper and lower limits\n",
    "    y_u = y_pred[0]\n",
    "    y_l = y_pred[1]\n",
    "\n",
    "    # Calculate hard captured vector\n",
    "    K_HU = torch.max(torch.zeros(y_true.size()).to(device), torch.sign(y_u - y_true))\n",
    "    K_HL = torch.max(torch.zeros(y_true.size()).to(device), torch.sign(y_true - y_l))\n",
    "    K_H = torch.mul(K_HU, K_HL)\n",
    "\n",
    "    # Calculate soft captured vector\n",
    "    K_SU = torch.sigmoid(soften_ * (y_u - y_true))\n",
    "    K_SL = torch.sigmoid(soften_ * (y_true - y_l))\n",
    "    K_S = torch.mul(K_SU, K_SL)\n",
    "\n",
    "    MPIW_c = torch.sum(torch.mul((y_u - y_l), K_H)) / (torch.sum(K_H) + 0.0001)\n",
    "    PICP_S = torch.mean(K_S)\n",
    "\n",
    "    # Calculate loss (Eq. 15 QD paper)\n",
    "    Loss_S = MPIW_c + beta_ * len(y_true) / (alpha_ * (1 - alpha_)) * torch.max(torch.zeros(1).to(device),\n",
    "                                                                                (1 - alpha_) - PICP_S)\n",
    "    return Loss_S"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now consider the following toy dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Training data\n",
    "X = torch.tensor([2, 3, 5, 6])    # Inputs\n",
    "Y = torch.tensor([4, 6, 10, 12])  # Ground-truth"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given the initial network parameters, when $X$ passes through the network, only three out of four generated PIs cover the target values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted upper bounds YU = [ 7.  8. 10. 11.]\n",
      "Predicted lower bounds YL = [3.2 4.7 7.7 9.2]\n",
      "----------------------------------------\n",
      "Targets = [ 4  6 10 12]\n",
      "----------------------------------------\n",
      "Covered? k = [ True  True  True False]\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "net = ToyNet()\n",
    "YU, YL = net(X)\n",
    "YUn, YLn, Yn = np.round(YU.tolist(), 2), np.round(YL.tolist(), 2), np.round(Y.tolist(), 2)\n",
    "print('Predicted upper bounds YU = ' + str(YUn))\n",
    "print('Predicted lower bounds YL = ' + str(YLn))\n",
    "print('----------------------------------------')\n",
    "print('Targets = ' + str(Yn))\n",
    "print('----------------------------------------')\n",
    "print('Covered? k = ' + str((Yn <= YUn) & (YLn <= Yn)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's calculate the loss value and perform backpropagation to update the network weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 tensor(0.9912)\n",
      "b1 tensor(4.9933)\n",
      "w2 tensor(1.5178)\n",
      "b2 tensor(0.2074)\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer (SGD in this case)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Calculate the loss using the \"QD\" loss function\n",
    "loss = QD_objective([YU, YL], Y, device='cpu')\n",
    "\n",
    "# Perform the backward pass\n",
    "optimizer.zero_grad()  # Zero the gradients to avoid accumulation\n",
    "loss.backward()\n",
    "\n",
    "# Update the training parameters using SGD\n",
    "optimizer.step()\n",
    "\n",
    "# Display the updated parameters\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After updating the weights, evaluate the new PIs generated by the network.\n",
    "\n",
    "**IMPORTANT:**\n",
    "\n",
    "Note that $k_4 = 0$ (i.e., the fourth sample was not captured initially), and thus its PI width was not considered when calculating the $MPIW_{capt}$ value. However, this did not prevent that the PIs generated for the non-captured sample decreased along with those from the captured samples. The following results indicate that the width PIs of the first three samples were reduced and, notably, the width of the fourth sample was also reduced despite not capturing the corresponding target value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original predicted upper bounds YU = [ 7.  8. 10. 11.]\n",
      "original predicted lower bounds YL = [3.2 4.7 7.7 9.2]\n",
      "Targets = [ 4  6 10 12]\n",
      "----------------------------------------\n",
      "Covered? k = tensor([ True,  True,  True, False])\n",
      "\n",
      "New predicted upper bounds YU = [ 6.98  7.97  9.95 10.94]\n",
      "New predicted lower bounds YL = [3.24 4.76 7.8  9.31]\n",
      "----------------------------------------\n",
      "Targets = [ 4  6 10 12]\n",
      "----------------------------------------\n",
      "Covered? k = [ True  True  True False]\n"
     ]
    }
   ],
   "source": [
    "# New forward pass\n",
    "YU2, YL2 = net(X)\n",
    "YU2n, YL2n = np.round(YU2.tolist(), 2), np.round(YL2.tolist(), 2)\n",
    "print('Original predicted upper bounds YU = ' + str(np.round(YUn.tolist(), 2)))\n",
    "print('original predicted lower bounds YL = ' + str(np.round(YLn.tolist(), 2)))\n",
    "print('Targets = ' + str(np.round(Y.tolist(), 2)))\n",
    "print('----------------------------------------')\n",
    "print('Covered? k = ' + str((Y <= YU) & (YL <= Y)))\n",
    "print('\\nNew predicted upper bounds YU = ' + str(np.round(YU2n.tolist(), 2)))\n",
    "print('New predicted lower bounds YL = ' + str(np.round(YL2n.tolist(), 2)))\n",
    "print('----------------------------------------')\n",
    "print('Targets = ' + str(np.round(Yn.tolist(), 2)))\n",
    "print('----------------------------------------')\n",
    "print('Covered? k = ' + str((Yn <= YUn) & (YLn <= Yn)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
